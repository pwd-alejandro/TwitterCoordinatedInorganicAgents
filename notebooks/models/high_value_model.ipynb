{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Basic operations\n",
    "from __future__ import division\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import gc\n",
    "# To be able to import from lib directory\n",
    "sys.path.append(r'..\\lib\\\\')\n",
    "\n",
    "#do not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Basic data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import plotly for visualization\n",
    "import chart_studio.plotly as py\n",
    "import plotly.offline as pyoff\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "\n",
    "# Import machine learning related libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import classification_report,confusion_matrix, SCORERS, f1_score, fbeta_score, precision_score, recall_score, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC as svc\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Encoders\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder \n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "\n",
    "# Smoteen\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "# LOCAL LIBRARIES\n",
    "import SQL_connection as Jsc\n",
    "import data_treatment as Adt\n",
    "\n",
    "user='ALEJANDRO.LOZADA@RAPPI.COM'\n",
    "password='zD13LqOhZDcPkEpgXacv8pA1'\n",
    "# Defines the sql connection\n",
    "con = Jsc.snowflake_connect(user=user,password=password)\n",
    "# Initializes pyoff on notebook mode\n",
    "pyoff.init_notebook_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_launch = ''' \n",
    "SELECT T0.*\n",
    "FROM [Jcountry]_writable.[Jtable] T0\n",
    "WHERE DATE_FEATURE_PRED::DATE = '[Jdate]'::DATE\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_update_predict(query_launch_=query_launch,\n",
    "                          features_table_name='hvu_daily_feat',\n",
    "                          predict_table_name= 'a',\n",
    "                          countries={'CO':5, 'PE':5}):\n",
    "    start_time = time.time()\n",
    "    today = datetime.date.today()\n",
    "    yesterday = today + datetime.timedelta(days=-1)\n",
    "\n",
    "    for country in zip(countries.keys(), countries.values()):\n",
    "        query_launch = query_launch_.replace('[Jtable]', features_table_name)\n",
    "        query_launch = query_launch.replace('[Jdate]', str(yesterday))\n",
    "        query_launch = query_launch.replace('[Jcountry]', country[0])\n",
    "\n",
    "        try:\n",
    "            alc_model = Jsc.pandas_df_from_snowflake_query(con, query_launch)\n",
    "            print('Try')\n",
    "        except ConnectionTimeOut as error:\n",
    "            alc_model = None\n",
    "            print('falló')\n",
    "            pass\n",
    "\n",
    "        if alc_model is not None:\n",
    "            print('is not none ' + country[0])\n",
    "            alc_model.columns = [i.upper() for i in alc_model.columns]\n",
    "            numeric_feat = [num for num in alc_model.columns if 'ZZZ' in num]\n",
    "            cate_feat_encode = [cat for cat in alc_model.columns if 'KKK' in cat]\n",
    "            extra_feat = [extra for extra in alc_model.columns if 'EXX' in extra]\n",
    "\n",
    "            alc_treat = alc_model[numeric_feat + cate_feat_encode + extra_feat]\n",
    "\n",
    "            alc_treat[numeric_feat] = alc_treat[numeric_feat].astype(float)\n",
    "\n",
    "            # Treat numeric null values\n",
    "            #alc_treat = adt.nan_numeric(alc_treat, numeric_feat)\n",
    "            #alc_treat = adt.nan_cate(alc_treat, cate_feat_encode)\n",
    "            #alc_treat = adt.nan_cate(alc_treat, extra_feat, replacement='-1')\n",
    "\n",
    "        else:\n",
    "            print('is none')\n",
    "            continue\n",
    "    return alc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alc_model = create_update_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alc_model['LOG_VERIFICATION_CODE_RATIO_ZZZ'].isnull().all():\n",
    "    print('wtf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "~alc_model['LOG_VERIFICATION_CODE_RATIO_ZZZ'].isnull().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     6,
     11,
     18
    ]
   },
   "outputs": [],
   "source": [
    "#Functions\n",
    "def delete(con,country,name):\n",
    "    quer = 'DROP TABLE [Jcountry]_WRITABLE.' + str(name)\n",
    "    quer = quer.replace('[Jcountry]',country)\n",
    "    Jsc.execute_snowflake_query(con, quer, with_cursor=False)\n",
    "    \n",
    "def excecutef(quer,country,command,con):\n",
    "    quer = quer.replace('[Jstatement]',command)\n",
    "    quer = quer.replace('[Jcountry]',country)\n",
    "    Jsc.execute_snowflake_query(con, quer, with_cursor=False)\n",
    "    \n",
    "def generate_data(query, \n",
    "                  conection, \n",
    "                  reset=False, \n",
    "                  countries = ['CO','BR','MX','CL','AR','PE','UY'], \n",
    "                  time_windows = ['1','3']):\n",
    "    start_time = time.time()\n",
    "    #statements\n",
    "    if reset:\n",
    "        con = conection\n",
    "        for Jcountry in countries:\n",
    "            print(\"--- Se originó centinela para \" + Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "            delete(con,Jcountry,'HIGH_VALUE_DATA')\n",
    "            print(\"--- Se borró la tabla para \" + Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "            for Jtarget in time_windows:\n",
    "                queryc = query\n",
    "                queryc = queryc.replace('[Jvertical]',Jtarget)\n",
    "                print(\"--- Se cargó el query para \" + Jtarget + \" ---\")\n",
    "                print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "\n",
    "                try:\n",
    "                    excecutef(queryc,Jcountry,insert,con)\n",
    "                    print(\"--- Se insertaron filas para \" + Jvertical + \" ---\")\n",
    "                    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "\n",
    "                except:               \n",
    "                    excecutef(queryc,Jcountry,create,con)\n",
    "                    print(\"--- Se originó la tabla con \" + Jvertical + \" --- \\nSu nombre es \"+str(Jcountry)+\"_WRITABLE.VERTICAL_CROSS_DATA_BACKUP\")\n",
    "                    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "    else:\n",
    "        insert = 'INSERT INTO [Jcountry]_WRITABLE.HIGH_VALUE_DATA'\n",
    "        create = 'CREATE TABLE [Jcountry]_WRITABLE.HIGH_VALUE_DATA AS'\n",
    "        con = conection\n",
    "        for Jcountry in countries:\n",
    "            print(\"--- Se originó centinela para \" + Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "            for Jtarget in time_windows:\n",
    "                queryc = query\n",
    "                queryc = queryc.replace('[Jtarget]',Jtarget)\n",
    "                print(\"--- Se cargó el query para \" + Jtarget + \" ---\")\n",
    "                print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "                try:\n",
    "                    excecutef(queryc,Jcountry,insert,con)\n",
    "                    print(\"--- Se insertaron filas para \" + Jtarget + \" ---\")\n",
    "                    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "\n",
    "                except:   \n",
    "                    print('Ya voy a crear')\n",
    "                    excecutef(queryc,Jcountry,create,con)\n",
    "                    print(\"--- Se originó la tabla con \" + Jtarget + \". n\\Su nombre es\"+str(Jcountry)+\"_WRITABLE.VERTICAL_CROSS_DATA_BACKUP ---\")\n",
    "                    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "                    \n",
    "def save_reporting(Jcountry, \n",
    "                   y_true, \n",
    "                   x_test , \n",
    "                   model, \n",
    "                   th, \n",
    "                   today, \n",
    "                   model_name, \n",
    "                   iterable, \n",
    "                   path='../data_1/', \n",
    "                   matrix_labels=[0,1]):\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc, balanced_accuracy_score\n",
    "    import six\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    \n",
    "    bbox=[0, 0, 1, 1]\n",
    "    \n",
    "    y_pre = model.predict_proba(x_test)[:,1]\n",
    "    \n",
    "    p = y_pre.copy()\n",
    "    p[p<th] = 0\n",
    "    p[p>=th] = 1\n",
    "    \n",
    "    #Balanced Accuracy\n",
    "    Adt.save(Jcountry, balanced_accuracy_score(y_true, p), path , model_name+'/'+iterable+'/Reporting','balanced_acc'+'_'+today)\n",
    "    \n",
    "    fullReport = PdfPages(path+Jcountry+'/'+model_name+'/'+iterable+'/Reporting/full_report'+'_'+today+\".pdf\")\n",
    "\n",
    "\n",
    "    #Adt.save(Jcountry, classification_report(p,y_true), path , model_name+'/'+iterable+'/Reporting','class_report'+'_'+today)\n",
    "\n",
    "    #Whole report - I\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    fig.suptitle('Reporte de resultados para EXP - 2019-12-20', fontsize=16, y=1.02)\n",
    "    ax1 = plt.subplot(2,2,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "    ax3 = plt.subplot(2,2,2)\n",
    "    \n",
    "    \n",
    "    #ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pre)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, lw=1, alpha=0.3,label='(AUC = %0.2f)'%roc_auc)\n",
    "    ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    ax1.set_xlim([-0.05, 1.05])\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC curve ')\n",
    "    ax1.legend(loc='best')\n",
    "    \n",
    "   \n",
    "    \n",
    "    #Lift Chart\n",
    "    df_dict = {'actual': list (y_true), 'pred': list(y_pre)}\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    pred_ranks = pd.qcut(df['pred'].rank(method='first'), 100, labels=False)\n",
    "    actual_ranks = pd.qcut(df['actual'].rank(method='first'), 100, labels=False)\n",
    "    pred_percentiles = df.groupby(pred_ranks).mean()\n",
    "    actual_percentiles = df.groupby(actual_ranks).mean()\n",
    "    \n",
    "    ax2.set_title('Lift Chart',y=1)\n",
    "    ax2.plot(np.arange(.01, 1.01, .01), np.array(pred_percentiles['pred']),\n",
    "             color='darkorange', lw=2, label='Prediction')\n",
    "    ax2.plot(np.arange(.01, 1.01, .01), np.array(pred_percentiles['actual']),\n",
    "             color='navy', lw=2, linestyle='--', label='Actual')\n",
    "    ax2.set_ylabel('Target Average')\n",
    "    ax2.set_xlabel('Population Percentile')\n",
    "    ax2.set_xlim([0.0, 1])\n",
    "    ax2.set_ylim([0,0.05+max([max(np.array(pred_percentiles['pred'])),max(np.array(pred_percentiles['actual']))])])\n",
    "    ax2.legend(loc=\"best\")\n",
    "    \n",
    "    \n",
    "    #Confusion Matrix\n",
    "    array = confusion_matrix(y_true, p, matrix_labels)\n",
    "    df_cm = pd.DataFrame(array, index = [i for i in matrix_labels],\n",
    "                                columns = [i for i in matrix_labels])\n",
    "    sns.heatmap(df_cm, annot=True, cmap='Blues',ax=ax3)\n",
    "    ax3.set_title('Confusion Matrix ',y=1.08)\n",
    "    #ax3.set_xticklabels([''] + matrix_labels)\n",
    "    #ax3.set_yticklabels([''] + matrix_labels)\n",
    "    ax3.set_xlabel('Predicted')\n",
    "    ax3.set_ylabel('True')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.2)\n",
    "    fullReport.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "\n",
    "    #Whole report II\n",
    "    fig1 = plt.figure(figsize=(12,12))\n",
    "    fig1.suptitle('Reporte de resultados para EXP - 2019-12-20', fontsize=16, y=1.02)\n",
    "    ax4 = plt.subplot(2,2,1)\n",
    "    ax5 = plt.subplot(2,2,2)\n",
    "    ax6 = plt.subplot(2,1,2)\n",
    "    \n",
    "    ax4.axis('off')\n",
    "    ax5.axis('off')\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    #Important features\n",
    "    #All\n",
    "    arg_s = (-model.feature_importances_).argsort()\n",
    "    data = pd.DataFrame(zip(x_test.columns[arg_s] ,model.feature_importances_[arg_s]),columns = ['COLUMNA','IMPORTANCIA'])\n",
    "    data.IMPORTANCIA = round(data.IMPORTANCIA,3)\n",
    "    data.COLUMNA = data['COLUMNA'].map(lambda x: x.replace('KKK','').replace('ZZZ','').replace('EXX','')) \n",
    "    data.COLUMNA = data['COLUMNA'].map(lambda x: '\\n '.join(x[i:i + 15] for i in range(0, len(x), 15))) \n",
    "    \n",
    "    #Best 10\n",
    "    ax4.set_title('Best Features',y=1.02)\n",
    "    best = ax4.table(cellText=data.head(10).values, bbox=bbox, colLabels=data.columns)\n",
    "    best.auto_set_font_size(False)\n",
    "    best.set_fontsize(14)\n",
    "    \n",
    "    #Worst 10\n",
    "    ax5.set_title('Worst Features',y=1.02)\n",
    "    worst = ax5.table(cellText=data.tail(10).values, bbox=bbox, colLabels=data.columns)\n",
    "    worst.auto_set_font_size(False)\n",
    "    worst.set_fontsize(14)\n",
    "    \n",
    "    \n",
    "    #Classification report\n",
    "    report = classification_report(y_true, p, output_dict=True)\n",
    "    clas = pd.DataFrame(report).transpose()\n",
    "    for i in clas.columns:\n",
    "        clas[i] = round(clas[i],3)\n",
    "        \n",
    "    ax6.set_title('Classification Report',y=1.02)\n",
    "    clasi = ax6.table(cellText=clas.values, bbox=bbox, rowLabels=clas.index, colLabels=clas.columns)\n",
    "    \n",
    "    clasi.auto_set_font_size(False)\n",
    "    clasi.set_fontsize(14)\n",
    "\n",
    "    #Tables pretty\n",
    "    for mpl_table in [best,worst,clasi]:\n",
    "        for k, cell in six.iteritems(mpl_table._cells):\n",
    "            cell.set_edgecolor('w')\n",
    "            if k[0] == 0 or k[1] < 0:\n",
    "                cell.set_text_props(weight='bold', color='w')\n",
    "                cell.set_facecolor('#40466e')\n",
    "            else:\n",
    "                cell.set_facecolor(['#f1f1f2', 'w'][k[0]%len(['#f1f1f2', 'w']) ])\n",
    "\n",
    "    fig1.tight_layout()\n",
    "    \n",
    "    \n",
    "    fig1.subplots_adjust(left=0.2, bottom=None, right=None, top=None, wspace=0.2, hspace=0.2)\n",
    "    fullReport.savefig()\n",
    "    plt.close()\n",
    "    fullReport.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6,
     44
    ]
   },
   "outputs": [],
   "source": [
    "#query_microzones\n",
    "query_microzones = '''\n",
    "[Jstatement]\n",
    "\n",
    "WITH\n",
    "\n",
    "PARAMS AS (\n",
    "\n",
    "    SELECT 14 AS HISTORY_MONTHS,\n",
    "           8 AS TARGET_WINDOW,\n",
    "           2 AS BACK_WINDOW,\n",
    "           16 AS TOP_DEFINITION,\n",
    "           'BR' AS COUNTRY,\n",
    "           17 AS TASA_MX,\n",
    "           2800 AS TASA_CO,\n",
    "           46 AS TASA_AR,\n",
    "           31 AS TASA_UY,\n",
    "           3.2 AS TASA_PE,\n",
    "           620 AS TASA_CL,\n",
    "           3.2 AS TASA_BR,\n",
    "           0.1 AS AVG_TAKE_RATE\n",
    "    )\n",
    "\n",
    ",COMMISSIONS AS (\n",
    "\n",
    "        SELECT COUNTRY\n",
    "              ,ORDER_ID\n",
    "              ,MARKUP_USD\n",
    "              ,SERVICE_FEE_USD\n",
    "              ,VALUE_COMMISSION_STORE_USD\n",
    "              ,VALUE_COMMISSION_RESTAURANT_USD\n",
    "              ,RAPPICASH_SURCHARGE_USD\n",
    "              ,OTHER_SURCHARGES_USD\n",
    "              ,MARKUP_USD +\n",
    "                SERVICE_FEE_USD +\n",
    "                VALUE_COMMISSION_STORE_USD +\n",
    "                VALUE_COMMISSION_RESTAURANT_USD +\n",
    "                RAPPICASH_SURCHARGE_USD +\n",
    "                OTHER_SURCHARGES_USD AS COMMISSIONS\n",
    "        FROM GLOBAL_FINANCES.GLOBAL_ORDER_DETAILS\n",
    "        WHERE COUNTRY ILIKE (SELECT COUNTRY FROM PARAMS)\n",
    "\n",
    "    )\n",
    "\n",
    ",DISCOUNTS  AS (\n",
    "\n",
    "    SELECT SOURCE_TABLE AS COUNTRY\n",
    "          ,ORDER_ID\n",
    "          , zeroifnull(PROMOT_DISC_RETENTION) AS PROMOT_DISC_RETENTION\n",
    "          , zeroifnull(PROMOT_DISCOUNTS_REACTIVATION) AS PROMOT_DISCOUNTS_REACTIVATION\n",
    "          , zeroifnull(PROMOT_DISCOUNTS_COMPENSA) AS PROMOT_DISCOUNTS_COMPENSA\n",
    "          , zeroifnull(PROMOT_DISC_RETENTION) +\n",
    "            zeroifnull(PROMOT_DISCOUNTS_REACTIVATION) +\n",
    "            zeroifnull(PROMOT_DISCOUNTS_COMPENSA) AS DISCOUNTS_TO_REVENUE\n",
    "    FROM BR_WRITABLE.TBL_STG_FACT_UE_MKT A\n",
    "    LEFT JOIN GLOBAL_FINANCES.TBL_DIM_GEOGRAPHY_T1 B ON (A.SK_GEOGRAPHY = B.SK_GEOGRAPHY)\n",
    "    WHERE SOURCE_TABLE  ILIKE (SELECT COUNTRY FROM PARAMS)\n",
    "\n",
    "     )\n",
    "\n",
    ",SEMIFINAL AS ( \n",
    "    SELECT '[Jdate]'::DATE AS DATE\n",
    "           ,MICROZONE_ID\n",
    "           --REVENUE--\n",
    "           ,AVG(REVENUE) AS AVG_REV_ORD\n",
    "           ,MEDIAN(REVENUE) AS MED_REV_ORD\n",
    "           ,STDDEV(REVENUE) AS STD_REV_ORD\n",
    "           ,SUM(REVENUE) AS TOT_REV\n",
    "           --ORDERS MICRO--\n",
    "           ,COUNT(DISTINCT ORDER_ID) AS TOT_ORD\n",
    "           --USERS MICRO--\n",
    "           ,COUNT(DISTINCT APPLICATION_USER_ID) AS TOT_USERS       \n",
    "           --USERS-ORD-REV MICRO--\n",
    "           ,ROUND(TOT_ORD/TOT_USERS,4) AS AVG_ORD_USER\n",
    "           ,ROUND(TOT_REV/TOT_USERS,4) AS AVG_REV_USER\n",
    "\n",
    "    FROM( \n",
    "        SELECT  O.APPLICATION_USER_ID\n",
    "               ,O.COUNTRY\n",
    "               ,O.ORDER_ID\n",
    "               ,COUNT(DISTINCT O.ORDER_ID) OVER (PARTITION BY O.APPLICATION_USER_ID) AS NUM_ORD\n",
    "               ,O.MICROZONE_ID\n",
    "               ,O.CREATED_AT\n",
    "               ,ZEROIFNULL(MARKUP_USD) AS MARKUP_USD\n",
    "               ,ZEROIFNULL(SERVICE_FEE_USD) AS SERVICE_FEE_USD\n",
    "               ,ZEROIFNULL(VALUE_COMMISSION_STORE_USD) AS VALUE_COMMISSION_STORE_USD\n",
    "               ,ZEROIFNULL(VALUE_COMMISSION_RESTAURANT_USD) AS VALUE_COMMISSION_RESTAURANT_USD\n",
    "               ,ZEROIFNULL(RAPPICASH_SURCHARGE_USD) AS RAPPICASH_SURCHARGE_USD\n",
    "               ,ZEROIFNULL(OTHER_SURCHARGES_USD) AS OTHER_SURCHARGES_USD\n",
    "               ,ZEROIFNULL(COMMISSIONS) AS COMMISSION\n",
    "               ,ZEROIFNULL(DISCOUNTS_TO_REVENUE) AS DISCOUNTS_TO_REVENUE\n",
    "               ,ZEROIFNULL(COMMISSIONS) + ZEROIFNULL(DISCOUNTS_TO_REVENUE) AS REVENUE\n",
    "        FROM GLOBAL_FINANCES.GLOBAL_ORDERS AS O\n",
    "        LEFT JOIN COMMISSIONS AS C ON (O.COUNTRY = C.COUNTRY AND O.ORDER_ID = C.ORDER_ID)\n",
    "        LEFT JOIN DISCOUNTS AS D ON (O.COUNTRY = D.COUNTRY AND O.ORDER_ID = D.ORDER_ID)\n",
    "        WHERE O.STATE_TYPE = 'FINISHED'\n",
    "        AND COALESCE(O.STORE_TYPE, 'WHIMP') <> 'grin'\n",
    "        AND NOT IS_SUBSCRIPTION\n",
    "        AND GMV_USD > 0\n",
    "        AND O.COUNTRY ILIKE (SELECT COUNTRY FROM PARAMS)\n",
    "        AND O.CREATED_AT::DATE <= '[Jdate]'::DATE\n",
    "        AND O.CREATED_AT::DATE >= DATEADD('MONTH', -(SELECT BACK_WINDOW FROM PARAMS), '[Jdate]'::DATE)::DATE)\n",
    "        GROUP BY 1, 2 )\n",
    "        \n",
    "SELECT T0.*\n",
    "      ,CASE WHEN PERC_MICRO_REV IN (5) AND PERC_MICRO_ORD IN (4,5) THEN 1 ELSE 0 END AS HV_MICROZONE\n",
    "FROM ( \n",
    "SELECT T0.*\n",
    "      ,NTILE(5) OVER (PARTITION BY NULL ORDER BY AVG_REV_USER) AS PERC_MICRO_REV\n",
    "      ,NTILE(5) OVER (PARTITION BY NULL ORDER BY AVG_ORD_USER) AS PERC_MICRO_ORD\n",
    "FROM SEMIFINAL T0 ) T0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Microzone data\n",
    "insert = 'INSERT INTO BR_WRITABLE.high_value_user_revenue_data_microzone_final'\n",
    "create = 'CREATE TABLE BR_WRITABLE.high_value_user_revenue_data_microzone_final AS'\n",
    "\n",
    "date_stop = datetime.date(2019,6,29)\n",
    "date_start =datetime.date(2019,6,29) #2019-01-01\n",
    "missing_days = date_stop - date_start\n",
    "missing_days = 1#missing_days.days\n",
    "\n",
    "\n",
    "for i in range(1, missing_days + 1):\n",
    "    query = query_microzones\n",
    "    date = date_start + datetime.timedelta(days=i)\n",
    "    query = query.replace('[Jdate]', str(date))\n",
    "    try:\n",
    "        queryi = query.replace('[Jstatement]',insert)\n",
    "        Jsc.execute_snowflake_query(con, queryi)\n",
    "        print('-- Se intentará insertar por primera vez en: ' +str(date)+ ' --')\n",
    "    except:\n",
    "        queryc = query.replace('[Jstatement]',create)\n",
    "        Jsc.execute_snowflake_query(con, queryc)\n",
    "        print('-- Se creó por primera vez en: '+str(date)+ ' --')\n",
    "        pass\n",
    "    print('-- Insertado con éxito para: ' + str(date) + ' --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#qh\n",
    "query_high = '''\n",
    "\n",
    "SELECT T0.*,\n",
    "       LEFT(RIGHT(try_to_number(APPLICATION_USER_ID),UNIFORM(2,3,HOUR(CURRENT_TIMESTAMP))),2) + LEFT(RIGHT(UNIFORM(1.00::FLOAT,100::FLOAT,HOUR(CURRENT_TIMESTAMP)),5),2) AS RANDOM\n",
    "FROM [Jcountry]_WRITABLE.[Jtable] T0\n",
    "WHERE [Jiterable_name] = 6--[Jiterable]\n",
    "ORDER BY RANDOM\n",
    "limit 200000\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#query_test\n",
    "query_test = '''\n",
    "\n",
    "SELECT T0.*\n",
    "FROM [Jcountry]_WRITABLE.HIGH_VALUE_USER_DATA_BACKTEST T0\n",
    "WHERE TIME_WINDOW = [Jtime]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_th\n",
    "query_th = '''\n",
    "\n",
    "SELECT *\n",
    "FROM BR_WRITABLE.high_value_user_revenue_data\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_th = Jsc.pandas_df_from_snowflake_query(con, query_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat = ALC_th.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1,
     44
    ]
   },
   "outputs": [],
   "source": [
    "#EXCLUDE\n",
    "EXCLUDE = ['QTY_SELECT_LOGIN_TOTAL_ZZZ',\n",
    "           'QTY_SELECT_LOGIN_ZZZ',\n",
    "           'ORDER_PLACED_CONFIRMED_ERROR_FOUND_ZZZ',\n",
    "           'QTY_ORDER_PC_ERROR_TOTAL_ZZZ',\n",
    "           'QTY_COUPON_ERROR_TOTAL_ZZZ',\n",
    "           'QTY_COUPON_ERROR_ZZZ',\n",
    "           'COMPLETE_SIGN_ERROR_FOUND_ZZZ',\n",
    "           'QTY_COMPLETE_SIGN_ERROR_TOTAL_ZZZ',\n",
    "           'COMPLETE_REGISTRATION_FOUND_ZZZ',\n",
    "           'SESSIONS_FOUND_EXX',\n",
    "           'QTY_COMPLETE_LOGIN_ERROR_ZZZ',\n",
    "           'COMPLETE_LOGIN_FOUND_ZZZ',\n",
    "           'QTY_COMPLETE_LOGIN_ZZZ',\n",
    "           'ADDRESS_CHECK_ERROR_FOUND_ZZZ',\n",
    "           'QTY_ADDRESS_CHECK_ERROR_ZZZ',\n",
    "           'ADDRESS_CHANGED_FOUND_ZZZ',\n",
    "           'QTY_ADDRESS_CHANGED_ZZZ',\n",
    "           'ADD_ADDRESS_ERROR_FOUND_ZZZ',\n",
    "           'QTY_VIEW_HOME_ERROR_ZZZ',\n",
    "           'COUPON_ERROR_FOUND_ZZZ',\n",
    "           'QTY_COMPLETE_REGISTRATION_ZZZ',\n",
    "           'COMPLETE_REGISTER_ERROR_FOUND_ZZZ',\n",
    "           'QTY_COMP_REGI_ERROR_TOTAL_ZZZ',\n",
    "           'QTY_COMPLETE_REGISTER_ERROR_ZZZ',\n",
    "           'COMPLETE_LOGIN_ERROR_FOUND_ZZZ',\n",
    "           'QTY_ADD_ADDRESS_ERROR_TOTAL_ZZZ',\n",
    "           'MODE_OS_KKK',\n",
    "           'QTY_ADD_ADDRESS_ERROR_ZZZ',\n",
    "           'VERIFICATION_CODE_ERROR_FOUND_ZZZ',\n",
    "           'QTY_VERIFICATION_CODE_SUCCESS_ZZZ',\n",
    "           'CARD_BRAND_AGG_KKK', 'ACTUAL_TIP_SHARE_ZZZ',\n",
    "           'DIF_ORGANIC_SHARE_ZZZ', 'MODE_PAYMENT_METHOD_KKK', 'TRIES_ZZZ',\n",
    "           'DIF_FIRST_DEVICE_YEAR_ZZZ', 'REGISTER_TRIES_ZZZ',\n",
    "           'NUMBER_OF_CC_ZZZ', 'LOG_AVG_DURATION_MS_AVG_TOTAL_ZZZ',\n",
    "           'WEEK_REGISTER_CC_EXX', 'VIEW_HOME_ERROR_FOUND_ZZZ',\n",
    "           'QTY_VIEW_HOME_ERROR_TOTAL_ZZZ',\n",
    "           'VERIFICATION_CODE_SUCCESS_FOUND_ZZZ',\n",
    "           'LOG_VERIFICATION_CODE_RATIO_ZZZ', 'VERIFICATION_CODE_RATIO_ZZZ',\n",
    "           'QTY_VERIFICATION_CODE_ERROR_ZZZ', 'QTY_COMPLETE_SIGN_ERROR_ZZZ',\n",
    "           'RELATIVE_DIFF_MS_DURATION_ZZZ', 'MAX_TIP_SHARE_ZZZ',\n",
    "           'SAME_COUNTRY_EXX',\n",
    "           'HIGH_VALUE_USER_EXX']\n",
    "\n",
    "NEW_OUT = ['AVG_REV_ORD_MIC_ZZZ',\n",
    "           'MED_REV_ORD_MIC_ZZZ',\n",
    "           'TOT_REV_MIC_ZZZ',\n",
    "           'TOT_ORD_MIC_ZZZ',\n",
    "           'TOT_USERS_MIC_ZZZ',\n",
    "           'PERC_MICRO_REV_MIC_EXX',\n",
    "           'PERC_MICRO_ORD_MIC_EXX',\n",
    "           'HV_MICROZONE_MIC_EXX',\n",
    "           'AVG_ORD_USER_MIC_ZZZ',  # Micro\n",
    "           'AVG_REV_USER_MIC_ZZZ',  # Micro\n",
    "           'STD_REV_ORD_MIC_ZZZ']  # Micro\n",
    "\n",
    "EXCLUDE = EXCLUDE + NEW_OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [num for num in ALC_treat.columns if 'ZZZ' in num]\n",
    "cate_feat_encode = [cat for cat in ALC_treat.columns if 'KKK' in cat]\n",
    "extra_feat = [extra for extra in ALC_treat.columns if 'EXX' in extra]\n",
    "        \n",
    "numeric_feat = [num for num in numeric_feat if num not in EXCLUDE]\n",
    "cate_feat_encode = [cat for cat in cate_feat_encode if cat not in EXCLUDE]\n",
    "extra_feat = [extra for extra in extra_feat if extra not in EXCLUDE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat = Adt.Nan_Numeric(ALC_treat,numeric_feat)\n",
    "ALC_treat = Adt.Nan_Cate(ALC_treat,cate_feat_encode)\n",
    "ALC_treat = Adt.Nan_Cate(ALC_treat,extra_feat,replacement=-1)\n",
    "\n",
    "X = ALC_treat[numeric_feat+extra_feat+cate_feat_encode]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country='BR'\n",
    "time='5'\n",
    "path_cate_high = '../data_1/{}/high_value_users_new/{}/cate_high_2020-04-23'.format(country,str(time))\n",
    "path_cate_low = '../data_1/{}/high_value_users_new/{}/cate_low_2020-04-23'.format(country,str(time))\n",
    "path_sc = '../data_1/{}/high_value_users_new/{}/scaler_2020-04-23'.format(country,str(time))\n",
    "path_mod = '../data_1/{}/high_value_users_new/{}/mod_2020-04-23'.format(country,str(time))\n",
    "path_th = '../data_1/{}/high_value_users_new/{}/th_2020-04-23'.format(country,str(time))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "cate_high = Adt.read(path_cate_high)\n",
    "cate_low = Adt.read(path_cate_low)\n",
    "sc = Adt.read(path_sc)\n",
    "mod = Adt.read(path_mod)\n",
    "th = Adt.read(path_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[numeric_feat] = pd.DataFrame(sc.transform(X[numeric_feat]), columns=numeric_feat, index = X.index)\n",
    "X = cate_high.transform(X)\n",
    "X = cate_low.transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mod.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat['Y_PRED'] =y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat['Y_PRED_rank'] = pd.qcut(ALC_treat['Y_PRED'].rank(method='first'), 100, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_50 = min(ALC_treat[ALC_treat['Y_PRED_rank']>=50].Y_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat[ALC_treat['Y_PRED_rank']>=50].shape[0]/ALC_treat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = ALC_treat['Y_PRED']\n",
    "y_predicted[y_predicted<th_50] = 0\n",
    "y_predicted[y_predicted>=th_50] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ALC_treat['Y_PRED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat['MONTH'] = [i.month for i in ALC_treat.FIRST_ACTUAL_ORDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ALC_treat['MONTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "all_feat = numeric_feat+extra_feat+cate_feat_encode\n",
    "\n",
    "for i in all_feat:\n",
    "    sns.distplot(ALC_treat[i])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat[ALC_treat.APPLICATION_USER_ID==39344]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ALC_treat.HIGH_VALUE_USER_EXX,y_predicted ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    \n",
    "    query1 = query_test.replace('[Jcountry]',country)\n",
    "    \n",
    "    query2 = query1.replace('[Jtime]',str(1))\n",
    "    ALC_test_ = Jsc.pandas_df_from_snowflake_query(con, query2)\n",
    "    print('Cargados los datos crudos para ' + country)\n",
    "    \n",
    "    numeric_feat = [num for num in ALC_test_.columns if 'ZZZ' in num]\n",
    "    cate_feat_encode = [cat for cat in ALC_test_.columns if 'KKK' in cat]\n",
    "    extra_feat = [extra for extra in ALC_test_.columns if 'EXX' in extra]\n",
    "        \n",
    "    numeric_feat = [num for num in numeric_feat if num not in extra]\n",
    "    \n",
    "\n",
    "    ALC_treat_ = ALC_test_[numeric_feat + cate_feat_encode + extra_feat]\n",
    "\n",
    "    ALC_treat_[numeric_feat] = ALC_treat_[numeric_feat].astype(float)\n",
    "\n",
    "    #Treat numeric null values\n",
    "    ALC_treat_ = Adt.Nan_Numeric(ALC_treat_,numeric_feat)\n",
    "    ALC_treat_ = Adt.Nan_Cate(ALC_treat_,cate_feat_encode)\n",
    "    ALC_treat_ = Adt.Nan_Cate(ALC_treat_,extra_feat,replacement=-1)\n",
    "        \n",
    "    X_ = ALC_treat_.drop(\"HIGH_VALUE_USER_EXX\",axis=1)\n",
    "    \n",
    "    \n",
    "    for time in time_windows:\n",
    "        \n",
    "        query = query_test.replace('[Jcountry]',country)\n",
    "        query = query.replace('[Jtime]',str(time))\n",
    "        \n",
    "        print(query)\n",
    "\n",
    "        ALC_test = Jsc.pandas_df_from_snowflake_query(con, query)\n",
    "        \n",
    "        print('Cargados los datos: ' +str(time))\n",
    "        \n",
    "        path_cate_high = '../data_1/{}/high_value_users/{}/cate_high_2020-03-27'.format(country,str(time))\n",
    "        path_cate_low = '../data_1/{}/high_value_users/{}/cate_low_2020-03-27'.format(country,str(time))\n",
    "        path_sc = '../data_1/{}/high_value_users/{}/scaler_2020-03-27'.format(country,str(time))\n",
    "        path_mod = '../data_1/{}/high_value_users/{}/mod_2020-03-27'.format(country,str(time))\n",
    "        path_th = '../data_1/{}/high_value_users/{}/th_2020-03-27'.format(country,str(time))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        cate_high = Adt.read(path_cate_high)\n",
    "        cate_low = Adt.read(path_cate_low)\n",
    "        sc = Adt.read(path_sc)\n",
    "        mod = Adt.read(path_mod)\n",
    "        th = Adt.read(path_th)\n",
    "        \n",
    "        print('Cargados los pkl')\n",
    "        \n",
    "\n",
    "        X = X_.copy()\n",
    "        X[numeric_feat] = pd.DataFrame(sc.transform(X[numeric_feat]), columns=numeric_feat, index = X.index)\n",
    "        X = cate_high.transform(X)\n",
    "        X = cate_low.transform(X)\n",
    "        \n",
    "        y = ALC_test[\"HIGH_VALUE_USER_EXX\"]\n",
    "        \n",
    "        print('Transformaciones listas')\n",
    "        \n",
    "        \n",
    "        save_reporting(country, \n",
    "                        y, \n",
    "                        X , \n",
    "                        mod, \n",
    "                        th, \n",
    "                        today='back_test_cry_4', \n",
    "                        model_name='high_value_users', \n",
    "                        iterable=str(time), \n",
    "                        path='../data_1/', \n",
    "                        matrix_labels=[0,1])\n",
    "        \n",
    "        \n",
    "        print('Guardado el reporting')\n",
    "        \n",
    "        ALC_test_['PRED_' + str(time)] = mod.predict_proba(X)[:, 1]\n",
    "\n",
    "        ALC_test_['TRH_' + str(time)] = [th] * ALC_test.shape[0]\n",
    "        \n",
    "        ALC_test_['REAL_HVU_' + str(time)] = ALC_test[\"HIGH_VALUE_USER_EXX\"]\n",
    "        \n",
    "        print(ALC_test.columns)\n",
    "        \n",
    "    export = [i for i in ALC_test_.columns if 'PRED_' in i] + \\\n",
    "             [i for i in ALC_test_.columns if 'TRH_' in i] + \\\n",
    "             [i for i in ALC_test_.columns if 'REAL_HVU_' in i] + \\\n",
    "             ['APPLICATION_USER_ID'] + ['FIRST_ACTUAL_ORDER']\n",
    "    \n",
    "    '''\n",
    "    resp = Jsc.write_snowflake_table(data=ALC_test_[export],\n",
    "                                        table_name='high_value_user_pred',\n",
    "                                        Jcountry = country,\n",
    "                                        user=user,\n",
    "                                        password = password,\n",
    "                                        if_exists_then_wat='replace')\n",
    "    '''                                            \n",
    "    print('Exportado a HIGH_VALUE_USER_PRED para ' + country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     155,
     173
    ]
   },
   "outputs": [],
   "source": [
    "#More_Functions\n",
    "\n",
    "def save_reporting(Jcountry, \n",
    "                   y_true, \n",
    "                   x_test , \n",
    "                   model, \n",
    "                   th, \n",
    "                   today, \n",
    "                   model_name, \n",
    "                   iterable, \n",
    "                   path='../data_1/', \n",
    "                   matrix_labels=[0,1]):\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc, balanced_accuracy_score\n",
    "    import six\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    \n",
    "    bbox=[0, 0, 1, 1]\n",
    "    \n",
    "    y_pre = model.predict_proba(x_test)[:,1]\n",
    "    \n",
    "    p = y_pre.copy()\n",
    "    p[p<th] = 0\n",
    "    p[p>=th] = 1\n",
    "    \n",
    "    #Balanced Accuracy\n",
    "    Adt.save(Jcountry, balanced_accuracy_score(y_true, p), path , model_name+'/'+iterable+'/Reporting','balanced_acc'+'_'+today)\n",
    "    \n",
    "    fullReport = PdfPages(path+Jcountry+'/'+model_name+'/'+iterable+'/Reporting/full_report'+'_'+today+\".pdf\")\n",
    "\n",
    "\n",
    "    #Adt.save(Jcountry, classification_report(p,y_true), path , model_name+'/'+iterable+'/Reporting','class_report'+'_'+today)\n",
    "\n",
    "    #Whole report - I\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    fig.suptitle('Reporte de resultados para EXP - 2019-12-20', fontsize=16, y=1.02)\n",
    "    ax1 = plt.subplot(2,2,1)\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "    ax3 = plt.subplot(2,2,2)\n",
    "    \n",
    "    \n",
    "    #ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pre)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, lw=1, alpha=0.3,label='(AUC = %0.2f)'%roc_auc)\n",
    "    ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    ax1.set_xlim([-0.05, 1.05])\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC curve ')\n",
    "    ax1.legend(loc='best')\n",
    "    \n",
    "   \n",
    "    \n",
    "    #Lift Chart\n",
    "    df_dict = {'actual': list (y_true), 'pred': list(y_pre)}\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    pred_ranks = pd.qcut(df['pred'].rank(method='first'), 100, labels=False)\n",
    "    actual_ranks = pd.qcut(df['actual'].rank(method='first'), 100, labels=False)\n",
    "    pred_percentiles = df.groupby(pred_ranks).mean()\n",
    "    actual_percentiles = df.groupby(actual_ranks).mean()\n",
    "    \n",
    "    ax2.set_title('Lift Chart',y=1)\n",
    "    ax2.plot(np.arange(.01, 1.01, .01), np.array(pred_percentiles['pred']),\n",
    "             color='darkorange', lw=2, label='Prediction')\n",
    "    ax2.plot(np.arange(.01, 1.01, .01), np.array(pred_percentiles['actual']),\n",
    "             color='navy', lw=2, linestyle='--', label='Actual')\n",
    "    ax2.set_ylabel('Target Average')\n",
    "    ax2.set_xlabel('Population Percentile')\n",
    "    ax2.set_xlim([0.0, 1])\n",
    "    ax2.set_ylim([0,0.05+max([max(np.array(pred_percentiles['pred'])),max(np.array(pred_percentiles['actual']))])])\n",
    "    ax2.legend(loc=\"best\")\n",
    "    \n",
    "    \n",
    "    #Confusion Matrix\n",
    "    array = confusion_matrix(y_true, p, matrix_labels)\n",
    "    df_cm = pd.DataFrame(array, index = [i for i in matrix_labels],\n",
    "                                columns = [i for i in matrix_labels])\n",
    "    sns.heatmap(df_cm, annot=True, cmap='Blues',ax=ax3)\n",
    "    ax3.set_title('Confusion Matrix ',y=1.08)\n",
    "    #ax3.set_xticklabels([''] + matrix_labels)\n",
    "    #ax3.set_yticklabels([''] + matrix_labels)\n",
    "    ax3.set_xlabel('Predicted')\n",
    "    ax3.set_ylabel('True')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.2)\n",
    "    fullReport.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "\n",
    "    #Whole report II\n",
    "    fig1 = plt.figure(figsize=(12,12))\n",
    "    fig1.suptitle('Reporte de resultados para EXP - 2019-12-20', fontsize=16, y=1.02)\n",
    "    ax4 = plt.subplot(2,2,1)\n",
    "    ax5 = plt.subplot(2,2,2)\n",
    "    ax6 = plt.subplot(2,1,2)\n",
    "    \n",
    "    ax4.axis('off')\n",
    "    ax5.axis('off')\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    #Important features\n",
    "    #All\n",
    "    arg_s = (-model.feature_importances_).argsort()\n",
    "    data = pd.DataFrame(zip(x_test.columns[arg_s] ,model.feature_importances_[arg_s]),columns = ['COLUMNA','IMPORTANCIA'])\n",
    "    data.IMPORTANCIA = round(data.IMPORTANCIA,3)\n",
    "    data.COLUMNA = data['COLUMNA'].map(lambda x: x.replace('KKK','').replace('ZZZ','').replace('EXX','')) \n",
    "    data.COLUMNA = data['COLUMNA'].map(lambda x: '\\n '.join(x[i:i + 15] for i in range(0, len(x), 15))) \n",
    "    \n",
    "    #Best 10\n",
    "    ax4.set_title('Best Features',y=1.02)\n",
    "    best = ax4.table(cellText=data.head(10).values, bbox=bbox, colLabels=data.columns)\n",
    "    best.auto_set_font_size(False)\n",
    "    best.set_fontsize(14)\n",
    "    \n",
    "    #Worst 10\n",
    "    ax5.set_title('Worst Features',y=1.02)\n",
    "    worst = ax5.table(cellText=data.tail(10).values, bbox=bbox, colLabels=data.columns)\n",
    "    worst.auto_set_font_size(False)\n",
    "    worst.set_fontsize(14)\n",
    "    \n",
    "    \n",
    "    #Classification report\n",
    "    report = classification_report(y_true, p, output_dict=True)\n",
    "    clas = pd.DataFrame(report).transpose()\n",
    "    for i in clas.columns:\n",
    "        clas[i] = round(clas[i],3)\n",
    "        \n",
    "    ax6.set_title('Classification Report',y=1.02)\n",
    "    clasi = ax6.table(cellText=clas.values, bbox=bbox, rowLabels=clas.index, colLabels=clas.columns)\n",
    "    \n",
    "    clasi.auto_set_font_size(False)\n",
    "    clasi.set_fontsize(14)\n",
    "\n",
    "    #Tables pretty\n",
    "    for mpl_table in [best,worst,clasi]:\n",
    "        for k, cell in six.iteritems(mpl_table._cells):\n",
    "            cell.set_edgecolor('w')\n",
    "            if k[0] == 0 or k[1] < 0:\n",
    "                cell.set_text_props(weight='bold', color='w')\n",
    "                cell.set_facecolor('#40466e')\n",
    "            else:\n",
    "                cell.set_facecolor(['#f1f1f2', 'w'][k[0]%len(['#f1f1f2', 'w']) ])\n",
    "\n",
    "    fig1.tight_layout()\n",
    "    \n",
    "    \n",
    "    fig1.subplots_adjust(left=0.2, bottom=None, right=None, top=None, wspace=0.2, hspace=0.2)\n",
    "    fullReport.savefig()\n",
    "    plt.close()\n",
    "    fullReport.close()\n",
    "    \n",
    "def export_train(query_promo:str =None, \n",
    "                 model_name:str =None,\n",
    "                 table_name_train:str =None,\n",
    "                 iterable_name:str =None,\n",
    "                 target_name:str =None,\n",
    "                 resample:bool =False,\n",
    "                 resampler:object =SMOTE(random_state=0),\n",
    "                 metric:object =f1_score,\n",
    "                 path:str ='../data_1/',\n",
    "                 model:object =XGBClassifier(), \n",
    "                 countries:list =['UY','MX','BR','AR','CL','PE','CO']):\n",
    "\n",
    "    start_time = time.time()\n",
    "    today = time.strftime('%Y-%m-%d', time.localtime(start_time))\n",
    "    print(\"--- Se hará el entrenamiento el día \" + today + \" ---\")\n",
    "    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "    \n",
    "    #exclude1\n",
    "    exclude1 =  ['QTY_SELECT_LOGIN_TOTAL_ZZZ',\n",
    "                 'QTY_SELECT_LOGIN_ZZZ',\n",
    "                 'ORDER_PLACED_CONFIRMED_ERROR_FOUND_ZZZ',\n",
    "                 'QTY_ORDER_PC_ERROR_TOTAL_ZZZ',\n",
    "                 'QTY_COUPON_ERROR_TOTAL_ZZZ',\n",
    "                 'QTY_COUPON_ERROR_ZZZ',\n",
    "                 'COMPLETE_SIGN_ERROR_FOUND_ZZZ',\n",
    "                 'QTY_COMPLETE_SIGN_ERROR_TOTAL_ZZZ',\n",
    "                 'COMPLETE_REGISTRATION_FOUND_ZZZ',\n",
    "                 'SESSIONS_FOUND_EXX',\n",
    "                 'QTY_COMPLETE_LOGIN_ERROR_ZZZ',\n",
    "                 'COMPLETE_LOGIN_FOUND_ZZZ',\n",
    "                 'QTY_COMPLETE_LOGIN_ZZZ',\n",
    "                 'ADDRESS_CHECK_ERROR_FOUND_ZZZ',\n",
    "                 'QTY_ADDRESS_CHECK_ERROR_ZZZ',\n",
    "                 'ADDRESS_CHANGED_FOUND_ZZZ',\n",
    "                 'QTY_ADDRESS_CHANGED_ZZZ',\n",
    "                 'ADD_ADDRESS_ERROR_FOUND_ZZZ',\n",
    "                 'QTY_VIEW_HOME_ERROR_ZZZ',\n",
    "                 'COUPON_ERROR_FOUND_ZZZ',\n",
    "                 'QTY_COMPLETE_REGISTRATION_ZZZ',\n",
    "                 'COMPLETE_REGISTER_ERROR_FOUND_ZZZ',\n",
    "                 'QTY_COMP_REGI_ERROR_TOTAL_ZZZ',\n",
    "                 'QTY_COMPLETE_REGISTER_ERROR_ZZZ',\n",
    "                 'COMPLETE_LOGIN_ERROR_FOUND_ZZZ',\n",
    "                 'QTY_ADD_ADDRESS_ERROR_TOTAL_ZZZ',\n",
    "                 'MODE_OS_KKK',\n",
    "                 'QTY_ADD_ADDRESS_ERROR_ZZZ',\n",
    "                 'VERIFICATION_CODE_ERROR_FOUND_ZZZ',\n",
    "                 'QTY_VERIFICATION_CODE_SUCCESS_ZZZ',\n",
    "                 'CARD_BRAND_AGG_KKK', \n",
    "                 'ACTUAL_TIP_SHARE_ZZZ',\n",
    "                 'DIF_ORGANIC_SHARE_ZZZ', \n",
    "                 'MODE_PAYMENT_METHOD_KKK', \n",
    "                 'TRIES_ZZZ',\n",
    "                 'DIF_FIRST_DEVICE_YEAR_ZZZ', \n",
    "                 'REGISTER_TRIES_ZZZ',\n",
    "                 'NUMBER_OF_CC_ZZZ', \n",
    "                 'LOG_AVG_DURATION_MS_AVG_TOTAL_ZZZ',\n",
    "                 'WEEK_REGISTER_CC_EXX', \n",
    "                 'VIEW_HOME_ERROR_FOUND_ZZZ',\n",
    "                 'QTY_VIEW_HOME_ERROR_TOTAL_ZZZ',\n",
    "                 'VERIFICATION_CODE_SUCCESS_FOUND_ZZZ',\n",
    "                 'LOG_VERIFICATION_CODE_RATIO_ZZZ', \n",
    "                 'VERIFICATION_CODE_RATIO_ZZZ',\n",
    "                 'QTY_VERIFICATION_CODE_ERROR_ZZZ', \n",
    "                 'QTY_COMPLETE_SIGN_ERROR_ZZZ',\n",
    "                 'RELATIVE_DIFF_MS_DURATION_ZZZ', \n",
    "                 'MAX_TIP_SHARE_ZZZ',\n",
    "                 'SAME_COUNTRY_EXX']\n",
    "\n",
    "    for Jcountry in countries:\n",
    "        query_promo = query_promo.replace('[Jcountry]',Jcountry)\n",
    "        query_promo = query_promo.replace('[Jtable]', table_name_train)\n",
    "        query_promo = query_promo.replace('[Jiterable_name]', iterable_name)\n",
    "        print(\"--- Se ha cargado el query para \" + Jcountry + \" ---\")\n",
    "        print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "        \n",
    "        query_iterable = '''\n",
    "\n",
    "        SELECT DISTINCT\n",
    "               [Jiterable_name]\n",
    "        FROM [Jcountry]_WRITABLE.[Jtable] T0\n",
    "\n",
    "        '''\n",
    "        \n",
    "        query_iterable = query_iterable.replace('[Jcountry]',Jcountry)\n",
    "        query_iterable = query_iterable.replace('[Jtable]', table_name_train)\n",
    "        query_iterable = query_iterable.replace('[Jiterable_name]', iterable_name)\n",
    "        print(\"--- Se ha cargado el query para \" + Jcountry + \" ---\")\n",
    "        print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "        \n",
    "        ALC_iterable = Jsc.pandas_df_from_snowflake_query(con, query_iterable)\n",
    "        print(\"--- Se corrió el query para \" + Jcountry + \" ---\")\n",
    "        print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "        \n",
    "        iterables = ALC_iterable[iterable_name].drop_duplicates().values\n",
    "        print(\"--- Se hará procesamiento para las iterables \" + iterables + \" ---\")\n",
    "        print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "        \n",
    "\n",
    "        for iterable in iterables:\n",
    "         \n",
    "            query_promo_ite = query_promo.replace('[Jiterable]',str(iterable))\n",
    "\n",
    "            ALC_model = Jsc.pandas_df_from_snowflake_query(con, query_promo_ite)\n",
    "            print(ALC_model.shape)\n",
    "            print(\"--- Se corrió el query para \" + Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "\n",
    "            numeric_feat = [num for num in ALC_model.columns if 'ZZZ' in num]\n",
    "            cate_feat_encode = [cat for cat in ALC_model.columns if 'KKK' in cat]\n",
    "            extra_feat = [extra for extra in ALC_model.columns if 'EXX' in extra]\n",
    "            \n",
    "            numeric_feat = [num for num in numeric_feat if num not in exclude1]\n",
    "            cate_feat_encode = [cat for cat in cate_feat_encode if cat not in exclude1]\n",
    "            extra_feat = [extra for extra in extra_feat if extra not in exclude1]\n",
    "\n",
    "            ALC_treat = ALC_model[numeric_feat + cate_feat_encode + extra_feat]\n",
    "            \n",
    "            del ALC_model\n",
    "            gc.collect()\n",
    "            \n",
    "            #Treat numeric null values\n",
    "            ALC_treat = Adt.Nan_Numeric(ALC_treat,numeric_feat)\n",
    "            print(\"--- Se llenaron vacíos numéricos con el promedio para \" + str(iterable) +' en '+ Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "            ALC_treat = Adt.Nan_Cate(ALC_treat,cate_feat_encode)\n",
    "            ALC_treat = Adt.Nan_Cate(ALC_treat,extra_feat,replacement=-1)\n",
    "            print(\"--- Se llenaron vacíos categóricos con el 'Otro' para \" + str(iterable) +' en '+ Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "\n",
    "            #Data processing\n",
    "            #Numeric scaling\n",
    "            sc = StandardScaler()\n",
    "            sc.fit(ALC_treat[numeric_feat])        \n",
    "            ALC_treat[numeric_feat] = pd.DataFrame(sc.transform(ALC_treat[numeric_feat]), columns=numeric_feat, index = ALC_treat.index)\n",
    "            print(\"--- Se hizo el escalamiento de variables numéricas para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "        \n",
    "            #Define X and Target\n",
    "            y = ALC_treat[target_name]\n",
    "            X = ALC_treat.drop([target_name],axis=1)\n",
    "            print(\"--- Se definió X y Y para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "            \n",
    "            del ALC_treat\n",
    "            gc.collect()\n",
    "            \n",
    "            #Split training and test\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 27, stratify=y)\n",
    "            print(\"--- Se separó entre train y test para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))   \n",
    "\n",
    "            print(sum(Y_test)/len(Y_test))\n",
    "\n",
    "            del X, y\n",
    "            gc.collect()\n",
    "            \n",
    "            #Category encoding\n",
    "            high_cardinality = []\n",
    "            low_cardinality = []\n",
    "            \n",
    "            for cat_column in cate_feat_encode:\n",
    "                cates = len(X_train[str(cat_column)].drop_duplicates().values)\n",
    "                if (cates/len(cate_feat_encode)) > 100:\n",
    "                    high_cardinality.append(cat_column)\n",
    "                else:\n",
    "                    low_cardinality.append(cat_column)\n",
    "                    \n",
    "            cate_low = CatBoostEncoder(cols = low_cardinality, drop_invariant=False)\n",
    "            cate_low.fit(X_train, Y_train, cols = low_cardinality)\n",
    "            X_train = cate_low.transform(X_train, Y_train)\n",
    "                    \n",
    "            cate_high = CatBoostEncoder(cols = high_cardinality, drop_invariant=False)\n",
    "            cate_high.fit(X_train, Y_train, cols = high_cardinality)\n",
    "            X_train = cate_high.transform(X_train, Y_train)\n",
    "            \n",
    "            \n",
    "            X_test = cate_high.transform(X_test)\n",
    "            X_test = cate_low.transform(X_test)\n",
    "            #cate_low = OneHotEncoder(use_cat_names=True)\n",
    "            #cate_low.fit(X,y,cols=cate_feat_encode)\n",
    "            \n",
    "            print(\"--- Se codificaron variables numéricas para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "\n",
    "\n",
    "            #Save scaler for prediction\n",
    "            Adt.save(Jcountry, sc, path, model_name+'/'+iterable, 'scaler_{}'.format(today))\n",
    "            Adt.save(Jcountry, cate_high, path, model_name+'/'+iterable, 'cate_high_{}'.format(today))\n",
    "            Adt.save(Jcountry, cate_low, path, model_name+'/'+iterable, 'cate_low_{}'.format(today))\n",
    "            print(\"--- Se guardaron el escalador de numéricas y el codificador de categóricas \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "            print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "            \n",
    "            if X_train.shape[0]>5000:\n",
    "                \n",
    "                print(sum(Y_train)/len(Y_train))\n",
    "            \n",
    "                if resample:\n",
    "                \n",
    "                    #Resampling\n",
    "                    sampler = resampler#SMOTE(random_state=0)\n",
    "                    \n",
    "                    X_train_r, Y_train_r = sampler.fit_resample(X_train, Y_train)\n",
    "                    \n",
    "                    X_train = pd.DataFrame(X_train_r, columns=X_train.columns)\n",
    "                    Y_train = Y_train_r\n",
    "                    \n",
    "                    print(\"--- Se hizo resampleo para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "                    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "                    pass\n",
    "                    #Smoteen\n",
    "                    #SMOTEENN(random_state=0)\n",
    "                \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "                #Training \n",
    "                # Model\n",
    "                mod = model#**init_mod_kwargs)\n",
    "                print(\"--- Se inicializó el modelo para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "                print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "\n",
    "                # Fit model\n",
    "                mod.fit(X_train,Y_train)\n",
    "                print(\"--- Se entrenó el modelo para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "                print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "                \n",
    "                # Reporting\n",
    "                # Maximize one metric\n",
    "                maximizer = metric\n",
    "                thresholds = []\n",
    "                p = mod.predict_proba(X_test)[:,1]\n",
    "                \n",
    "                # Threshold space\n",
    "                for thresh in np.arange(0.1, 0.801, 0.001):\n",
    "                    res = maximizer(Y_test, (p > thresh).astype(int))#, beta=0.01)\n",
    "                    thresholds.append([thresh, res])\n",
    "                    \n",
    "                thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                best_thresh =  np.round(thresholds[0][0], 4)\n",
    "                best_metric =  np.round(thresholds[0][1], 4)\n",
    "                \n",
    "                print(\"--- Best score: \" + str(best_thresh) +\". Metric: \" + str(best_metric) + \" en \"+Jcountry + \" ---\")\n",
    "                print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "                \n",
    "                #Save th for prediction\n",
    "                Adt.save(Jcountry, best_thresh, path, model_name+'/'+iterable,'th_{}'.format(today))   \n",
    "                print(\"--- Se guardó el treshold como el promedio de Y para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "                print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "                \n",
    "                save_reporting(Jcountry, \n",
    "                               Y_test, \n",
    "                               X_test, \n",
    "                               mod, \n",
    "                               best_thresh, \n",
    "                               today, \n",
    "                               model_name, \n",
    "                               iterable, \n",
    "                               path=path, \n",
    "                               matrix_labels=[0,1])\n",
    "                print(\"--- Se ha guardado el Reporting para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "                print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))           \n",
    "\n",
    "                #Save model for prediction\n",
    "                Adt.save(Jcountry, mod, path, model_name+'/'+iterable,'mod_{}'.format(today))\n",
    "                print(\"--- Se guardó el modelo para \" + iterable +' en '+ Jcountry + \" ---\")\n",
    "                print(\"--- %s seconds ---\\n\\n\\n\" % round((time.time() - start_time),2))\n",
    "                \n",
    "                del X_train, X_test, Y_train, Y_test\n",
    "                gc.collect()\n",
    "            else:\n",
    "                iterables = np.delete(iterables,np.argwhere(iterables==iterable))\n",
    "                print(\"--- No se hará modelo para \" + iterable +' en '+ Jcountry + \" por falta de información ---\")\n",
    "                print(\"--- %s seconds ---\\n\\n\\n\" % round((time.time() - start_time),2))\n",
    "                \n",
    "                \n",
    "        Adt.save(Jcountry, iterables, path, model_name+'/', 'iterables_{}'.format(today))\n",
    "        print(\"--- Se correrá un modelo para las iterablees \" + iterables + \" ---\")\n",
    "        print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mod_kwargs = { 'learning_rate':0.2\n",
    "                                   ,'n_estimators':50\n",
    "                                   ,'min_child_weight':1\n",
    "                                   ,'gamma':0\n",
    "                                   ,'subsample':0.8\n",
    "                                   ,'colsample_bytree':0.8\n",
    "                                   ,'objective':'binary:logistic'\n",
    "                                   ,'scale_pos_weight':1\n",
    "                                   ,'nthread':4 \n",
    "                                   ,'seed':1144}\n",
    "                \n",
    "param_test1 = {'scale_pos_weight':range(50,110,20)}\n",
    "                \n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(**init_mod_kwargs), \n",
    "                                    param_grid=param_test1, \n",
    "                                    scoring='roc_auc',\n",
    "                                    n_jobs=4,\n",
    "                                    iid=False, \n",
    "                                    cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'learning_rate':[10,50,100], \n",
    "             'n_estimators':[800, 1000, 1700]}    # parameters to be tries in the grid search\n",
    "fix_params = {'max_depth':4, \n",
    "              'min_samples_split':2, \n",
    "              'min_samples_leaf':1, \n",
    "              'subsample':1,\n",
    "              'max_features':'sqrt'}   #other parameters, fixed for the moment \n",
    "csv = GridSearchCV(GradientBoostingClassifier(**fix_params), cv_params, scoring = 'f1', cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.cv_results_, csv.best_params_, csv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#qh\n",
    "query_high = '''\n",
    "\n",
    "SELECT T0.*,\n",
    "       LEFT(RIGHT(try_to_number(APPLICATION_USER_ID),UNIFORM(2,3,HOUR(CURRENT_TIMESTAMP))),2) + LEFT(RIGHT(UNIFORM(1.00::FLOAT,100::FLOAT,HOUR(CURRENT_TIMESTAMP)),5),2) AS RANDOM\n",
    "FROM [Jcountry]_WRITABLE.[Jtable] T0\n",
    "--WHERE [Jiterable_name] = 6--[Jiterable]\n",
    "ORDER BY RANDOM\n",
    "limit 200000\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query_high.replace('[Jtable]','high_value_user_revenue_data_microzone_exp')\n",
    "q = q.replace('[Jcountry]','BR')\n",
    "ALC_grid = Jsc.pandas_df_from_snowflake_query(con, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query_high.replace('[Jtable]','high_value_user_revenue_data_microzone_exp')\n",
    "q = q.replace('[Jcountry]','BR')\n",
    "ALC = Jsc.pandas_df_from_snowflake_query(con, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_model = ALC_grid.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ALC_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#exclude1\n",
    "exclude1 = ['QTY_SELECT_LOGIN_TOTAL_ZZZ',\n",
    " 'QTY_SELECT_LOGIN_ZZZ',\n",
    " 'ORDER_PLACED_CONFIRMED_ERROR_FOUND_ZZZ',\n",
    " 'QTY_ORDER_PC_ERROR_TOTAL_ZZZ',\n",
    " 'QTY_COUPON_ERROR_TOTAL_ZZZ',\n",
    " 'QTY_COUPON_ERROR_ZZZ',\n",
    " 'COMPLETE_SIGN_ERROR_FOUND_ZZZ',\n",
    " 'QTY_COMPLETE_SIGN_ERROR_TOTAL_ZZZ',\n",
    " 'COMPLETE_REGISTRATION_FOUND_ZZZ',\n",
    " 'SESSIONS_FOUND_EXX',\n",
    "'QTY_COMPLETE_LOGIN_ERROR_ZZZ',\n",
    " 'COMPLETE_LOGIN_FOUND_ZZZ',\n",
    " 'QTY_COMPLETE_LOGIN_ZZZ',\n",
    " 'ADDRESS_CHECK_ERROR_FOUND_ZZZ',\n",
    " 'QTY_ADDRESS_CHECK_ERROR_ZZZ',\n",
    " 'ADDRESS_CHANGED_FOUND_ZZZ',\n",
    " 'QTY_ADDRESS_CHANGED_ZZZ',\n",
    " 'ADD_ADDRESS_ERROR_FOUND_ZZZ',\n",
    " 'QTY_VIEW_HOME_ERROR_ZZZ',\n",
    " 'COUPON_ERROR_FOUND_ZZZ',\n",
    "'QTY_COMPLETE_REGISTRATION_ZZZ',\n",
    " 'COMPLETE_REGISTER_ERROR_FOUND_ZZZ',\n",
    " 'QTY_COMP_REGI_ERROR_TOTAL_ZZZ',\n",
    " 'QTY_COMPLETE_REGISTER_ERROR_ZZZ',\n",
    " 'COMPLETE_LOGIN_ERROR_FOUND_ZZZ',\n",
    " 'QTY_ADD_ADDRESS_ERROR_TOTAL_ZZZ',\n",
    " 'MODE_OS_KKK',\n",
    " 'QTY_ADD_ADDRESS_ERROR_ZZZ',\n",
    " 'VERIFICATION_CODE_ERROR_FOUND_ZZZ',\n",
    " 'QTY_VERIFICATION_CODE_SUCCESS_ZZZ',\n",
    "'CARD_BRAND_AGG_KKK', 'ACTUAL_TIP_SHARE_ZZZ',\n",
    "       'DIF_ORGANIC_SHARE_ZZZ', 'MODE_PAYMENT_METHOD_KKK', 'TRIES_ZZZ',\n",
    "       'DIF_FIRST_DEVICE_YEAR_ZZZ', 'REGISTER_TRIES_ZZZ',\n",
    "       'NUMBER_OF_CC_ZZZ', 'LOG_AVG_DURATION_MS_AVG_TOTAL_ZZZ',\n",
    "       'WEEK_REGISTER_CC_EXX', 'VIEW_HOME_ERROR_FOUND_ZZZ',\n",
    "       'QTY_VIEW_HOME_ERROR_TOTAL_ZZZ',\n",
    "       'VERIFICATION_CODE_SUCCESS_FOUND_ZZZ',\n",
    "       'LOG_VERIFICATION_CODE_RATIO_ZZZ', 'VERIFICATION_CODE_RATIO_ZZZ',\n",
    "       'QTY_VERIFICATION_CODE_ERROR_ZZZ', 'QTY_COMPLETE_SIGN_ERROR_ZZZ',\n",
    "       'RELATIVE_DIFF_MS_DURATION_ZZZ', 'MAX_TIP_SHARE_ZZZ',\n",
    "       'SAME_COUNTRY_EXX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out = [ 'AVG_REV_ORD_MIC_ZZZ'\n",
    "            ,'MED_REV_ORD_MIC_ZZZ'\n",
    "            ,'TOT_REV_MIC_ZZZ'\n",
    "            ,'TOT_ORD_MIC_ZZZ'\n",
    "            ,'TOT_USERS_MIC_ZZZ'\n",
    "            ,'PERC_MICRO_REV_MIC_EXX'\n",
    "            ,'PERC_MICRO_ORD_MIC_EXX'\n",
    "            ,'HV_MICROZONE_MIC_EXX'\n",
    "            ,'AVG_ORD_USER_MIC_ZZZ' #Micro\n",
    "            ,'AVG_REV_USER_MIC_ZZZ' #Micro\n",
    "            ,'STD_REV_ORD_MIC_ZZZ'] #Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude1 = exclude1 + new_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental = [ 'PERC_FIRST_REVE'\n",
    "                ,'ACTUAL_FIRST_REVENUE']\n",
    "\n",
    "numeric_feat = [num for num in ALC_model.columns if 'ZZZ' in num]#+[experimental[1]]\n",
    "cate_feat_encode = [cat for cat in ALC_model.columns if 'KKK' in cat]\n",
    "extra_feat = [extra for extra in ALC_model.columns if 'EXX' in extra] #+[experimental[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [num for num in numeric_feat if num not in exclude1]\n",
    "cate_feat_encode = [cat for cat in cate_feat_encode if cat not in exclude1]\n",
    "extra_feat = [extra for extra in extra_feat if extra not in exclude1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat = ALC_model[numeric_feat + cate_feat_encode + extra_feat]\n",
    "\n",
    "            \n",
    "#Treat numeric null values\n",
    "ALC_treat = Adt.Nan_Numeric(ALC_treat,numeric_feat)\n",
    "\n",
    "ALC_treat = Adt.Nan_Cate(ALC_treat,cate_feat_encode)\n",
    "ALC_treat = Adt.Nan_Cate(ALC_treat,extra_feat,replacement=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(ALC_treat[numeric_feat])        \n",
    "ALC_treat[numeric_feat] = pd.DataFrame(sc.transform(ALC_treat[numeric_feat]), columns=numeric_feat, index = ALC_treat.index)\n",
    "        \n",
    "#Define X and Target\n",
    "y = ALC_treat['HIGH_VALUE_USER_EXX']\n",
    "X = ALC_treat.drop(['HIGH_VALUE_USER_EXX'],axis=1)\n",
    "            \n",
    "#Split training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 27, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_low = CatBoostEncoder(cols = cate_feat_encode, drop_invariant=False)\n",
    "cate_low.fit(X_train, Y_train, cols = cate_feat_encode)\n",
    "X_train = cate_low.transform(X_train, Y_train)\n",
    "                    \n",
    "#cate_high = LeaveOneOutEncoder(cols = high_cardinality, drop_invariant=False)\n",
    "#cate_high.fit(X_train, Y_train, cols = high_cardinality)\n",
    "#X_train = cate_high.transform(X_train)\n",
    "            \n",
    "            \n",
    "#X_test = cate_high.transform(X_test)\n",
    "X_test = cate_low.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "#from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "#pca = PCA(.95)\n",
    "#pca.fit(X_train)\n",
    "#X_train = pca.transform(X_train)\n",
    "#X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#oversampling\n",
    "#sampler = ADASYN(random_state=0)#ADASYN\n",
    "                    \n",
    "#X_train_r, Y_train_r = sampler.fit_resample(X_train, Y_train)\n",
    "                    \n",
    "#X_train = pd.DataFrame(X_train_r, columns=X_train.columns)\n",
    "#Y_train = Y_train_r\n",
    "\n",
    "\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Y_train)/len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(Y_test, mod.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximizer = fbeta_score\n",
    "thresholds = []\n",
    "p = mod.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Threshold space\n",
    "for thresh in np.arange(0.1, 0.801, 0.001):\n",
    "    res = maximizer(Y_test, (p > thresh).astype(int), beta=0.8)\n",
    "    thresholds.append([thresh, res])\n",
    "                    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "best_thresh =  np.round(thresholds[0][0], 4)\n",
    "best_metric =  np.round(thresholds[0][1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_thresh, best_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = best_thresh\n",
    "p=mod.predict_proba(X_test)[:,1]\n",
    "p[p<th]=0\n",
    "p[p>=th]=1\n",
    "print(classification_report(Y_test,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All\n",
    "arg_s = (-mod.feature_importances_).argsort()\n",
    "data1 = pd.DataFrame(zip(X_test.columns[arg_s] , mod.feature_importances_[arg_s]), columns = ['COLUMNA','IMPORTANCIA'])\n",
    "data1.IMPORTANCIA = round(data1.IMPORTANCIA,3)\n",
    "#data.COLUMNA = data['COLUMNA'].map(lambda x: x.replace('KKK','').replace('ZZZ','').replace('EXX','')) \n",
    "#data.COLUMNA = data['COLUMNA'].map(lambda x: '\\n '.join(x[i:i + 15] for i in range(0, len(x), 15))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.tail(20).COLUMNA.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "init = {'n_estimators':800}\n",
    "export_train(query_promo=query_high, \n",
    "             model_name='high_value_users_new', \n",
    "             table_name_train='high_value_user_revenue_data',\n",
    "             iterable_name='TIME_WINDOW',\n",
    "             target_name='HIGH_VALUE_USER_EXX',\n",
    "             resample = False,\n",
    "             resampler = SMOTEENN(random_state=0),\n",
    "             path='../data_1/',\n",
    "             model=GradientBoostingClassifier(**init),#XGBClassifier(), \n",
    "             countries=['BR', 'CO', 'MX', 'CL', 'AR', 'PE', 'UY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bactest tryout\n",
    "query_back = '''\n",
    "SELECT *\n",
    "FROM [Jcountry]_writable.high_value_user_revenue_data_backtest\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "countries = {'BR': 5, 'MX': 4, 'CO': 5, 'CL': 4, 'AR': 5}#{'BR': 5, 'MX': 4, 'CO': 5, 'PE': 5, 'CL': 4, 'AR': 5, 'UY': 5}\n",
    "start_time = time.time()\n",
    "today = time.strftime('%Y-%m-%d', time.localtime(start_time))\n",
    "print(\"--- Se hará el entrenamiento el día \" + today + \" ---\")\n",
    "print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "for country in zip(countries.keys(),countries.values()):\n",
    "    \n",
    "    query1 = query_back.replace('[Jcountry]',country[0])\n",
    "    \n",
    "    ALC_model = Jsc.pandas_df_from_snowflake_query(con, query1)\n",
    "    print('--- Cargados los datos crudos para ' + country[0] + ' ---')\n",
    "    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "    \n",
    "    numeric_feat = [num for num in ALC_model.columns if 'ZZZ' in num]\n",
    "    cate_feat_encode = [cat for cat in ALC_model.columns if 'KKK' in cat]\n",
    "    extra_feat = [extra for extra in ALC_model.columns if 'EXX' in extra]\n",
    "            \n",
    "    numeric_feat = [num for num in numeric_feat if num not in exclude1]\n",
    "    cate_feat_encode = [cat for cat in cate_feat_encode if cat not in exclude1]\n",
    "    extra_feat = [extra for extra in extra_feat if extra not in exclude1]\n",
    "\n",
    "    ALC_treat = ALC_model[numeric_feat + cate_feat_encode + extra_feat]\n",
    "\n",
    "    ALC_treat[numeric_feat] = ALC_treat[numeric_feat].astype(float)\n",
    "\n",
    "    #Treat numeric null values\n",
    "    ALC_treat = Adt.Nan_Numeric(ALC_treat,numeric_feat)\n",
    "    ALC_treat = Adt.Nan_Cate(ALC_treat,cate_feat_encode)\n",
    "    ALC_treat = Adt.Nan_Cate(ALC_treat,extra_feat,replacement=-1)\n",
    "        \n",
    "    X = ALC_treat.drop(\"HIGH_VALUE_USER_EXX\",axis=1)\n",
    "        \n",
    "    path_cate_high = '../data_1/{}/high_value_users_new/{}/cate_high_2020-04-23'.format(country[0],str(country[1]))\n",
    "    path_cate_low = '../data_1/{}/high_value_users_new/{}/cate_low_2020-04-23'.format(country[0],str(country[1]))\n",
    "    path_sc = '../data_1/{}/high_value_users_new/{}/scaler_2020-04-23'.format(country[0],str(country[1]))\n",
    "    path_mod = '../data_1/{}/high_value_users_new/{}/mod_2020-04-23'.format(country[0],str(country[1]))\n",
    "    path_th = '../data_1/{}/high_value_users_new/{}/th_2020-04-23'.format(country[0],str(country[1]))\n",
    "        \n",
    "        \n",
    "    cate_high = Adt.read(path_cate_high)\n",
    "    cate_low = Adt.read(path_cate_low)\n",
    "    sc = Adt.read(path_sc)\n",
    "    mod = Adt.read(path_mod)\n",
    "    th = Adt.read(path_th)\n",
    "        \n",
    "    print('--- Cargados los pkl'+ ' ---')\n",
    "    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "        \n",
    "    X[numeric_feat] = pd.DataFrame(sc.transform(X[numeric_feat]), columns=numeric_feat, index = X.index)\n",
    "    X = cate_high.transform(X)\n",
    "    X = cate_low.transform(X)\n",
    "        \n",
    "    y = ALC_treat[\"HIGH_VALUE_USER_EXX\"]\n",
    "        \n",
    "    print('--- Transformaciones listas'+ ' ---')\n",
    "    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "        \n",
    "        \n",
    "    save_reporting(country[0], \n",
    "                   y, \n",
    "                   X , \n",
    "                   mod, \n",
    "                   th, \n",
    "                   today='back_test', \n",
    "                   model_name='high_value_users_new', \n",
    "                   iterable=str(country[1]), \n",
    "                   path='../data_1/', \n",
    "                   matrix_labels=[0,1])\n",
    "        \n",
    "        \n",
    "    print('--- Guardado el reporting'+ ' ---')\n",
    "    print(\"--- %s seconds ---\\n\" % round((time.time() - start_time),2))\n",
    "        \n",
    "    ALC_model['PRED_' + str(country[1])] = mod.predict_proba(X)[:, 1]\n",
    "\n",
    "    ALC_model['TRH_' + str(country[1])] = [th] * ALC_model.shape[0]\n",
    "        \n",
    "    ALC_model['REAL_HVU_' + str(country[1])] = ALC_model[\"HIGH_VALUE_USER_EXX\"]\n",
    "        \n",
    "        \n",
    "    export = [i for i in ALC_model.columns if 'PRED_' in i] + \\\n",
    "             [i for i in ALC_model.columns if 'TRH_' in i] + \\\n",
    "             [i for i in ALC_model.columns if 'REAL_HVU_' in i] + \\\n",
    "             [i for i in ALC_model.columns if ('PREV' in i) & ('LAUNCH' not in i)] + \\\n",
    "             [i for i in ALC_model.columns if 'PORD' in i] + \\\n",
    "             [i for i in ALC_model.columns if 'PERC' in i] + \\\n",
    "             ['APPLICATION_USER_ID'] + ['FIRST_ACTUAL_ORDER', 'TIME_WINDOW']\n",
    "    \n",
    "    \n",
    "    resp = Jsc.write_snowflake_table(data=ALC_model[export],\n",
    "                                        table_name='high_value_user_pred_backtest',\n",
    "                                        Jcountry = country[0],\n",
    "                                        user=user,\n",
    "                                        password = password,\n",
    "                                        if_exists_then_wat='replace')\n",
    "    \n",
    "    del ALC_model\n",
    "    gc.collect()\n",
    "                                                \n",
    "    print('--- Exportado a high_value_user_pred_backtest para ' + country[0]+ ' ---')\n",
    "    print(\"--- %s seconds ---\\n\\n\" % round((time.time() - start_time),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_model[export].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_model = Jsc.pandas_df_from_snowflake_query(con, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_model.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_model.APPLICATION_USER_ID.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [num for num in ALC_model.columns if 'ZZZ' in num]\n",
    "cate_feat_encode = [cat for cat in ALC_model.columns if 'KKK' in cat]\n",
    "extra_feat = [extra for extra in ALC_model.columns if 'EXX' in extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat = ALC_model[numeric_feat + cate_feat_encode + extra_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat = Adt.Nan_Cate(ALC_treat,cate_feat_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ALC_treat['HIGH_VALUE_USER_EXX']\n",
    "X = ALC_treat.drop(['HIGH_VALUE_USER_EXX'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[cate_feat_encode].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Category encoding\n",
    "high_cardinality = []\n",
    "low_cardinality = []\n",
    "            \n",
    "for cat_column in cate_feat_encode:\n",
    "    cates = len(ALC_treat[str(cat_column)].drop_duplicates().values)\n",
    "    if (cates/len(cate_feat_encode)) > 0.25:\n",
    "        high_cardinality.append(cat_column)\n",
    "    else:\n",
    "        low_cardinality.append(cat_column)\n",
    "                    \n",
    "cate_high = LeaveOneOutEncoder(cols=high_cardinality+low_cardinality, drop_invariant=True)\n",
    "cate_high.fit(X,y,cols=high_cardinality+low_cardinality)\n",
    "X = cate_high.transform(X,y)\n",
    "            \n",
    "#cate_low = OneHotEncoder(use_cat_names=True)\n",
    "#cate_low.fit(X,y,cols=low_cadinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['a'] + []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [num for num in ALC_model.columns if 'ZZZ' in num]\n",
    "cate_feat_encode = [cat for cat in ALC_model.columns if 'KKK' in cat]\n",
    "extra_feat = [extra for extra in ALC_model.columns if 'EXX' in extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_treat = ALC_model[numeric_feat + cate_feat_encode + extra_feat]\n",
    "\n",
    "            #Treat numeric null values\n",
    "ALC_treat = Adt.Nan_Numeric(ALC_treat,numeric_feat)\n",
    "\n",
    "ALC_treat = Adt.Nan_Cate(ALC_treat,cate_feat_encode)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(ALC_treat[numeric_feat])        \n",
    "ALC_treat[numeric_feat] = pd.DataFrame(sc.transform(ALC_treat[numeric_feat]), columns=numeric_feat, index = ALC_treat.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ALC_treat['HIGH_VALUE_USER_EXX']\n",
    "X = ALC_treat.drop(['HIGH_VALUE_USER_EXX'],axis=1)\n",
    "            \n",
    "            \n",
    "            #Split training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 27, stratify=y)\n",
    "            \n",
    "\n",
    "            #Category encoding\n",
    "high_cardinality = []\n",
    "low_cardinality = []\n",
    "            \n",
    "for cat_column in cate_feat_encode:\n",
    "    cates = len(X_train[str(cat_column)].drop_duplicates().values)\n",
    "    if (cates/len(cate_feat_encode)) > 20:\n",
    "        high_cardinality.append(cat_column)\n",
    "        print(cat_column,cates/len(cate_feat_encode))\n",
    "    else:\n",
    "        low_cardinality.append(cat_column)\n",
    "        print(cat_column,cates/len(cate_feat_encode))\n",
    "                    \n",
    "#cate_high = LeaveOneOutEncoder(cols = high_cardinality, drop_invariant=True)\n",
    "#cate_high.fit(X_train, Y_train, cols = high_cardinality)\n",
    "\n",
    "#cate_low = LeaveOneOutEncoder(cols = low_cardinality, drop_invariant=False)\n",
    "#cate_low.fit(X_train, Y_train, cols = low_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=cate_low.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[low_cardinality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cate_feat_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ALC_model.APPLICATION_USER_ID.drop_duplicates().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mod.predict_proba(X_test)[:,1]\n",
    "\n",
    "y[y<th_org]=0\n",
    "y[y>=th_org]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [num for num in ALC_model.columns if 'ZZZ' in num]\n",
    "cate_feat_encode = [cat for cat in ALC_model.columns if 'KKK' in cat]\n",
    "extra_feat = [extra for extra in ALC_model.columns if 'EXX' in extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = ['AGE_RANGE_KKK','HIGH_VALUE_USER_EXX']\n",
    "why = ALC_model.copy()#[tl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transf = cate_high.transform(why[numeric_feat+cate_feat_encode+extra_feat].drop(['HIGH_VALUE_USER_EXX'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transf['TARGET'] = why['HIGH_VALUE_USER_EXX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(X_transf['HIGH_VALUE_USER_EXX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_model['LAST_VERTICAL_KKK'][ALC_model.LAST_VERTICAL_KKK=='GAMERS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transf[X_transf.index==151388].LAST_VERTICAL_KKK#.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='TARGET',y='LAST_VERTICAL_KKK',data=X_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = mod.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(Y_test,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#query_automation\n",
    "query_model = '''\n",
    "\n",
    "SELECT T0.*,\n",
    "       DAYNAME(CREATED_AT) AS DAY_WEEK,\n",
    "       CASE WHEN DAYNAME(CREATED_AT) IN ('Sat','Sun') THEN 1 ELSE 0 END AS WEEKEND,\n",
    "       LEFT(RIGHT(try_to_number(APPLICATION_USER_ID),UNIFORM(2,3,HOUR(CURRENT_TIMESTAMP))),2) + LEFT(RIGHT(UNIFORM(1.00::FLOAT,100::FLOAT,HOUR(CURRENT_TIMESTAMP)),5),2) AS RANDOM\n",
    "FROM [Jcountry]_WRITABLE.VERTICAL_CROSS_DATA T0\n",
    "WHERE RECENCY IS NOT NULL\n",
    "AND LENGTH(APPLICATION_USER_ID) >= 3\n",
    "AND LEFT(RIGHT(APPLICATION_USER_ID,2),1) NOT IN (1,5,9)\n",
    "ORDER BY RANDOM\n",
    "limit 1000000\n",
    "\n",
    "'''\n",
    "\n",
    "query_model_momentum = '''\n",
    "\n",
    "SELECT T0.*,\n",
    "       DAYNAME(CREATED_AT) AS DAY_WEEK,\n",
    "       CASE WHEN DAYNAME(CREATED_AT) IN ('Sat','Sun') THEN 1 ELSE 0 END AS WEEKEND,\n",
    "       LEFT(RIGHT(try_to_number(APPLICATION_USER_ID),UNIFORM(2,3,HOUR(CURRENT_TIMESTAMP))),2) + LEFT(RIGHT(UNIFORM(1.00::FLOAT,100::FLOAT,HOUR(CURRENT_TIMESTAMP)),5),2) AS RANDOM\n",
    "FROM [Jcountry]_WRITABLE.VERTICAL_CROSS_DATA T0\n",
    "WHERE RECENCY IS NOT NULL\n",
    "AND PEDIDOS <= 5\n",
    "AND LENGTH(APPLICATION_USER_ID) >= 3\n",
    "AND LEFT(RIGHT(APPLICATION_USER_ID,2),1) NOT IN (1,5,9)\n",
    "ORDER BY RANDOM\n",
    "limit 1000000\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#momentum\n",
    "export_train(query_model_momentum,\n",
    "             'cross_mod_momentum', \n",
    "             ['PEDIDOS','VERTICALS','GMV_CUMULATIVE','GMV_MEDIAN','GMV_MIN','GMV_RANGE','TICKET_CUMULATIVE','TICKET_MEDIAN','TICKET_MIN','TICKET_RANGE', 'TIP_CUMULATIVE' , 'TIP_MEDIAN' ,'TIP_MIN','TIP_RANGE','AVG_POND_NEW','MEDIAN_DIST','ORGANIC_RATIO','TIP_RATIO','RECENCY'], \n",
    "             ['DAY_WEEK','MODE_VERTICAL','FIRST_VERTICAL','LAST_VERTICAL','MODE_PAYMENT','FIRST_PAYMENT','LAST_PAYMENT','MODE_OS','FIRST_OS','LAST_OS'], \n",
    "             ['WEEKEND','CROSS_SELL'],\n",
    "             countries=['CO','BR','MX','PE','CL','AR']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conso\n",
    "export_train(query_model,\n",
    "             'cross_mod', \n",
    "             ['PEDIDOS','VERTICALS','GMV_CUMULATIVE','GMV_MEDIAN','GMV_MIN','GMV_RANGE','TICKET_CUMULATIVE','TICKET_MEDIAN','TICKET_MIN','TICKET_RANGE', 'TIP_CUMULATIVE' , 'TIP_MEDIAN' ,'TIP_MIN','TIP_RANGE','AVG_POND_NEW','MEDIAN_DIST','ORGANIC_RATIO','TIP_RATIO','RECENCY'], \n",
    "             ['DAY_WEEK','MODE_VERTICAL','FIRST_VERTICAL','LAST_VERTICAL','MODE_PAYMENT','FIRST_PAYMENT','LAST_PAYMENT','MODE_OS','FIRST_OS','LAST_OS'], \n",
    "             ['WEEKEND','CROSS_SELL'],\n",
    "             countries=['UY']\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beyond this point is how things get done before automation**\n",
    "\n",
    "**********************************************************************************\n",
    "\n",
    "Yo do not need to go there, but do it if you want\n",
    "\n",
    "\n",
    "*******************************************************************************\n",
    "\n",
    ".\n",
    "*******************************************************************************\n",
    "\n",
    "\n",
    ".\n",
    "**********************************************************************************\n",
    "\n",
    "\n",
    ":)\n",
    "**********************************************************************************\n",
    "\n",
    ".\n",
    "**********************************************************************************\n",
    "\n",
    "\n",
    ".\n",
    "**********************************************************************************\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_true, y_pre)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plot(fpr, tpr, lw=1, alpha=0.3,label='(AUC = %0.2f)'%roc_auc)\n",
    "plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "set_xlim([-0.05, 1.05])\n",
    "set_ylim([-0.05, 1.05])\n",
    "set_xlabel('False Positive Rate')\n",
    "set_ylabel('True Positive Rate')\n",
    "set_title('ROC curve ')\n",
    "legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(['Holi','Aaaa'])\n",
    "index = np.argwhere(a=='Holi')\n",
    "y = np.delete(a, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "\n",
    "SELECT *\n",
    "FROM BR_WRITABLE.HIGH_VALUE_DATA\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_model = Jsc.pandas_df_from_snowflake_query(con, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALC_model['CREATED_AT'] = pd.to_datetime(ALC_model.CREATED_AT)\n",
    "ALC_model['DAY_WEEK'] = [i.weekday() for i in ALC_model.CREATED_AT.values]\n",
    "ALC_model['WEEKEND'] = [0]*len(ALC_model['DAY_WEEK'])\n",
    "ALC_model['WEEKEND'][(ALC_model.DAY_WEEK == 5) & (ALC_model.DAY_WEEK == 6)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALC_model['CREATED_AT'] = pd.to_datetime(ALC_model.CREATED_AT)\n",
    "ALC_model['DAY_WEEK'] = [i.weekday() for i in ALC_model.CREATED_AT.values]\n",
    "ALC_model['WEEKEND'] = [0]*len(ALC_model['DAY_WEEK'])\n",
    "ALC_model['WEEKEND'][(ALC_model.DAY_WEEK == 5) & (ALC_model.DAY_WEEK == 6)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALC_model.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = ['PEDIDOS','VERTICALS','GMV_CUMULATIVE','GMV_MEDIAN','GMV_MIN','GMV_RANGE','TICKET_CUMULATIVE','TICKET_MEDIAN','TICKET_MIN','TICKET_RANGE', 'TIP_CUMULATIVE' , 'TIP_MEDIAN' ,'TIP_MIN','TIP_RANGE','AVG_POND_NEW','MEDIAN_DIST','ORGANIC_RATIO','TIP_RATIO','RECENCY']                                                \n",
    "cate_feat_encode = ['DAY_WEEK','MODE_VERTICAL','FIRST_VERTICAL','LAST_VERTICAL','MODE_PAYMENT','FIRST_PAYMENT','LAST_PAYMENT','MODE_OS','FIRST_OS','LAST_OS']\n",
    "extra_feat = ['WEEKEND','CROSS_SELL']\n",
    "\n",
    "ALC_treat = ALC_model[ALC_model.VERTICAL_SUB_GROUP=='EXP'][numeric_feat + cate_feat_encode + extra_feat]\n",
    "ALC_treat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treat numeric null values\n",
    "ALC_treat[numeric_feat].fillna(np.mean(ALC_treat[numeric_feat]),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Data pro\n",
    "#Numeric scaling\n",
    "#sc = StandardScaler()\n",
    "\n",
    "#sc.fit(ALC_treat[numeric_feat])\n",
    "ALC_treat[numeric_feat] = pd.DataFrame(sc.transform(ALC_treat[numeric_feat]), columns=numeric_feat, index = ALC_treat.index)\n",
    "\n",
    "#Define X and Target\n",
    "y = ALC_treat['CROSS_SELL']\n",
    "X = ALC_treat.drop(['CROSS_SELL'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#Category encoding\n",
    "#cate = OneHotEncoder()\n",
    "#cate.fit(X,y,cols=cate_feat_encode)\n",
    "X = cate.transform(X,y)\n",
    "\n",
    "\n",
    "#Save scaler for prediction\n",
    "#Adt.save(Jcountry, sc, '../Data/' , 'cross_model', 'scaler')\n",
    "#Adt.save(Jcountry, cate, '../Data/','cross_model', 'cate')\n",
    "\n",
    "#Split training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 27, stratify=y)\n",
    "\n",
    "''' \n",
    "#Resampling\n",
    "#Smote\n",
    "smote = SMOTE(random_state=0)\n",
    "X_tra_re, Y_tra_re = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "#Smoteen\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_tra_re1, Y_tra_re1 = smote_enn.fit_resample(X_train, Y_train)\n",
    "'''\n",
    "#Tresholds\n",
    "#th_org = sum(Y_train)/len(Y_train)\n",
    "\n",
    "#Save th for prediction\n",
    "#Adt.save(Jcountry, th_org, '../Data/' , 'cross_model','th')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adt.save(Jcountry, th_org, '../Data/' , 'cross_model','th')\n",
    "Adt.save(Jcountry, sc, '../Data/' , 'cross_model', 'scaler')\n",
    "Adt.save(Jcountry, cate, '../Data/','cross_model', 'cate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Model selection\n",
    "#Create an array of models\n",
    "models = []\n",
    "models.append((\"GB\",GradientBoostingClassifier()))\n",
    "models.append((\"RF\",RandomForestClassifier()))\n",
    "#models.append((\"SVC\",svc()))\n",
    "models.append((\"XGB\",XGBClassifier()))\n",
    "\n",
    "#Measure the some metric \n",
    "#To check what metrics are available use: sorted(SCORERS.keys())\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=3, random_state=22)\n",
    "    cv_result = cross_val_score(model,X_train,Y_train, cv = kfold, scoring = \"roc_auc\")\n",
    "    print(name, cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Training \n",
    "\n",
    "init_mod_kwargs = { 'n_estimators':30\n",
    "                   ,'min_samples_split':50\n",
    "                   ,'min_samples_leaf':8\n",
    "                   ,'max_depth':20\n",
    "                   ,'criterion':'gini'\n",
    "                   ,'max_features':'auto'\n",
    "                   ,'bootstrap':True\n",
    "                   ,'n_jobs':-1\n",
    "                   ,'random_state':2305\n",
    "                   ,'verbose':2 }\n",
    "# Model\n",
    "mod = XGBClassifier()#**init_mod_kwargs)\n",
    "\n",
    "# Fit model\n",
    "mod.fit(X_train,Y_train)\n",
    "\n",
    "#Save model for prediction\n",
    "#Adt.save(Jcountry, mod, '../../Data/' , 'cross_model','mod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Adt.read('../../Data/UY/cross_mod/verticals')\n",
    "#th_org = Adt.read('../../Data/UY/cross_mod/EXP/th')\n",
    "#cate = Adt.read('../../Data/UY/cross_mod/EXP/cate')\n",
    "#sc = Adt.read('../../Data/UY/cross_mod/EXP/scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.delete(mod,np.argwhere(mod=='MAR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordena las columnas de menor a mayor importancia\n",
    "arg_s = (-mod.feature_importances_).argsort()\n",
    "best_cols = pd.DataFrame(zip(X_train.columns[arg_s] ,mod.feature_importances_[arg_s]),columns = ['COLUMNA','IMPORTANCIA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.DataFrame([[1,2,3],[1,2,3]],columns=['A','B','C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mod.predict_proba(X_test)[:,1]\n",
    "p[p<th_org] = 0\n",
    "p[p>=th_org] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adt.save('UY', a, '../../Data/' , 'cross_mod'+'/'+'EXP','class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_insights as mli\n",
    "#xray = mli.ModelXRay(mod, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.choice([1,0],1000)\n",
    "Y_test = np.random.choice([1,0],1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = classification_report(p,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(p, Y_test, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Usuario\\conso_vertical_cross\\Data\\UY\\cross_mod\\LIC\\Reporting\\df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adt.save('UY', 'This is just to save coding time :)', '../../Data/' , 'cross_mod'+'/'+'EXP','class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plotLiftChart(Y_test, mod.predict_proba(X_test)[:,1],'../../Data/'+'UY'+'/'+'cross_mod'+'/'+'EXP'+'/'+'lift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(Y_test,mod.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_s = (-mod.feature_importances_).argsort()\n",
    "best_cols = list([X_test.columns[arg_s[:50]]][0])\n",
    "bad_cols = list([X_test.columns[arg_s[-50:]]][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Genera el reporte de performance de acuerdo a los puntos definidos anteriormente\n",
    "# Input: X: las variables predictivas, y: la variable target, model: el modelo entrenado\n",
    "def performance_report(X,y,model,folder=None):\n",
    "    # primero se obtiene la predicción\n",
    "    # Se obtiene la probabilidad en lugar de la clasificación final porque tiene más información y es más versátil\n",
    "    y_model = model.predict_proba(X)[:,1] # La probabilidad de la clase '1' (casualmente el índice 1 en la lista [0,1])\n",
    "    ##############################################################\n",
    "    ################ MAXIMIZACION DEL F1 SCORE ###################\n",
    "    ##############################################################\n",
    "    thresholds = []\n",
    "    # Recorre el espacio de thresholds (para en 0.4) dado el desbalanceo del target (~20%)\n",
    "    for thresh in np.arange(0.1, 0.401, 0.001):\n",
    "        # Calcula el f1 en el threshold actual\n",
    "        res = f1_score(y, (y_model > thresh).astype(int))\n",
    "        # añade los resultados a la lista\n",
    "        thresholds.append([thresh, res])\n",
    "    # Ordena los resultados de mayor F1 a menor F1\n",
    "    thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_thresh =  np.round(thresholds[0][0], 3)\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('El trheshold que maximiza el F1 score es: ', best_thresh)\n",
    "    print('-----------------------------------------------------------------')\n",
    "    # La prediccion en términos de clases\n",
    "    y_pred_class = (y_model > thresh).astype(int)\n",
    "    ##############################################################\n",
    "    ################ AREA UNDER THE (ROC) CURVE ##################\n",
    "    ##############################################################\n",
    "    # Tasa de falsos positivos (false positive rate) y tasa de verdaderos positivos (true positive rate)\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_model)\n",
    "    auc_val = auc(fpr, tpr)\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('El valor de AUC es: ', auc_val)\n",
    "    print('-----------------------------------------------------------------')\n",
    "    ##############################################################\n",
    "    ################### BALANCED_ACCURACY ########################\n",
    "    ############################################################## \n",
    "    bas = balanced_accuracy_score(y, y_pred_class)\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('El valor del Balanced Accuracy es: ', bas)\n",
    "    print('-----------------------------------------------------------------')\n",
    "    ##############################################################\n",
    "    ############## REPORTE DE CLASIFICACION ######################\n",
    "    ############################################################## \n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    classif_rep = classification_report(y, y_pred_class)\n",
    "    print(classif_rep)\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    # CREA LA CARPETA\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    cpickle.dump(classif_rep, open(folder+\"classification_report_\"+dt.today().strftime('%Y-%m-%d')+\".pickle\", 'wb'))  \n",
    "    # Crea la figura\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    #plt.gcf().add_subplot()\n",
    "    # Define los ejes\n",
    "    ax1 = plt.subplot(221)\n",
    "    ax3 = plt.subplot(212)\n",
    "    ax2 = plt.subplot(222)\n",
    "    ##############################################################\n",
    "    ################### GRAFICA LA ROC ###########################\n",
    "    ############################################################## \n",
    "    # Compute ROC curve and area the curve\n",
    "    ax1.plot(fpr, tpr, lw=1, alpha=0.3,label='(AUC = %0.3f)'%auc_val)\n",
    "    ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    ax1.set_xlim([-0.05, 1.05]) #plt.xlim\n",
    "    ax1.set_ylim([-0.05, 1.05]) \n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC curve ')\n",
    "    ax1.legend(loc='best')\n",
    "    ##############################################################\n",
    "    # MATRIZ DE CONFUSION ( con el threshold que maximiza el f1) #\n",
    "    ############################################################## \n",
    "    labels = [1, 0]\n",
    "    cm = confusion_matrix(y, y_pred_class, labels)\n",
    "    print(cm)\n",
    "    cax = ax2.matshow(np.log(cm))\n",
    "    fig.colorbar(cax)\n",
    "    ax2.set_xticklabels([''] + labels)\n",
    "    ax2.set_yticklabels([''] + labels)\n",
    "    ax2.set_xlabel('Actual Class')\n",
    "    ax2.set_ylabel('Predicted Class')\n",
    "    ##############################################################\n",
    "    ################ GRAFICA EL LIFT CHART #######################\n",
    "    ############################################################## \n",
    "    df_dict = {'actual': list (y), 'pred': list(y_model)}\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    pred_ranks = pd.qcut(df['pred'].rank(method='first'), 10, labels=False)\n",
    "    actual_ranks = pd.qcut(df['actual'].rank(method='first'), 10, labels=False)\n",
    "    pred_percentiles = df.groupby(pred_ranks).mean()\n",
    "    actual_percentiles = df.groupby(actual_ranks).mean()\n",
    "    ax3.set_title('Lift Chart')\n",
    "    ax3.plot(np.arange(.1, 1.1, .1), np.array(pred_percentiles['pred']),\n",
    "             color='darkorange', lw=2, label='Prediction')\n",
    "    ax3.plot(np.arange(.1, 1.1, .1), np.array(pred_percentiles['actual']),\n",
    "             color='navy', lw=2, linestyle='--', label='Actual')\n",
    "    ax3.set_ylabel('Target Average')\n",
    "    ax3.set_xlabel('Population Percentile')\n",
    "    ax3.set_xlim([0.0, 1.1])\n",
    "    ax3.set_ylim([0,0.05+max([max(np.array(pred_percentiles['pred'])),max(np.array(pred_percentiles['actual']))])])\n",
    "    ax3.legend(loc=\"best\")\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig(folder+\"roc_lift_confussion_report_\"+dt.today().strftime('%Y-%m-%d')+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_tree\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "plot_tree(mod, num_trees=0, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
